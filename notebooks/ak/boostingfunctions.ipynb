{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd96c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42e2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import myfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a13630",
   "metadata": {},
   "source": [
    "The import_train function contains the relative path to the folder containing the prepared folder within the repo. It reads and returns the train data with the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2abf001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((470744, 190), (39007, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, train_labels = myfunctions.import_train()\n",
    "train.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a5d31",
   "metadata": {},
   "source": [
    "The initial_prep function drops the date column 'S_2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3babab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = myfunctions.initial_prep(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73793fe6",
   "metadata": {},
   "source": [
    "The train_null_counter function groups the dataframe by customer ID and counts the number of nulls in each feature per customer. It returns a dataframe of new features containing this count. The counts are scaled with the StandardScaler from sklearn to return values of a similar range to the raw data. The function returns a dataframe of a new size: one row per customer. The function also returns the StandardScaler object for use on the validate/test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83355566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39007, 188)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_df, ss = myfunctions.train_null_counter(train)\n",
    "null_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9d764",
   "metadata": {},
   "source": [
    "The handle_categories function has the list of categorical columns found on the competition data page. It creates a dummy dataframe of these columns, including Null as a potential value for each feature. This function returns a new dataframe with one row per customer, with each value being the lat recorded value for each customer. It also returns the list of categorical columns, which must be fed into the next function to determine the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12b601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39007, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df, cat_columns = myfunctions.handle_categories(train)\n",
    "dummy_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a4e23",
   "metadata": {},
   "source": [
    "The cap_numerical_columns function uses the categorical columns list to create a list of numerical columns. The values for each numerical feature are clipped to a lower bound of -3 and an upper bound of 3. Given the features in the dataset are z-score normalized, clipping outliers to -3 and 3 still places them in an area occupying 0.1% of the area under the bell curve. The function returns a dataframe the same size as the original train dataframe. It also returns the list of numerical columns, which is used to impute the nulls in numerical columns and calculate aggregate features for these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8300a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, num_columns = myfunctions.cap_numerical_columns(train, cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659dbb5c",
   "metadata": {},
   "source": [
    "The impute_numerical_nulls function imputes -5 for all null values in the numerical columns. This value was chosen because it sits outside the range of possible values (-3 to 3), but not so far that it drastically skews some of the aggregate features. The optional third argument is set to -5 by default and can be changed when calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9473a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = myfunctions.impute_numerical_nulls(train, num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae94abc",
   "metadata": {},
   "source": [
    "The aggregate_features function creates many new features for the dataset: the minimum, maximum, median, standard deviation, last, and change for each feature by customer. The features are calculated for each numerical column and added as columns to a dataframe until all numerical columns have been handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68da4f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39007, 1063)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_features = myfunctions.aggregate_features(train, num_columns)\n",
    "agg_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfce8ad",
   "metadata": {},
   "source": [
    "At this point we have three dataframes containing our engineered features: the null counts, the dummy features for the categorical columns, and the aggregate features for the numerical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a423c4",
   "metadata": {},
   "source": [
    "The concat_dataframes function combines the three feature dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbe5e9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39007, 1271)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final = myfunctions.concat_dataframes(agg_features, dummy_df, null_df)\n",
    "train_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f54e43",
   "metadata": {},
   "source": [
    "At this point, I will cache this version of my features. It took roughly 30 minutes to execute the functions in this notebook and I don't want to take that amount of time to reach this point again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b601937",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.to_csv('train_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c693fdd",
   "metadata": {},
   "source": [
    "I will repeat the preparation on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e453393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = '../../data/prepared/'\n",
    "    \n",
    "valid = pd.read_csv(base_url + 'val_data.csv')\n",
    "    \n",
    "valid_labels = pd.read_csv(base_url + 'val_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f3afb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = myfunctions.initial_prep(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebf682d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_null = myfunctions.valid_null_counter(valid, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f38b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dummies, cat_columns = myfunctions.handle_categories(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e08a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid, num_columns = myfunctions.cap_numerical_columns(valid, cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24ffe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = myfunctions.impute_numerical_nulls(valid, num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f56fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_agg = myfunctions.aggregate_features(valid, num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c5e5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_agg.set_index('customer_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39cea072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214152, 1271)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_final = concat_dataframes(valid_agg, valid_dummies, valid_null)\n",
    "valid_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c09954df",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_final.to_csv('valid_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f6183",
   "metadata": {},
   "source": [
    "With both prepared dataframes cached to my local machine, I can move forward with training using the validation subset as the evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c034196",
   "metadata": {},
   "source": [
    "First I will do a left join of the target labels onto my feature dataframes, to ensure the features and the target will be on the same row for all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9af6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete = train_final.merge(train_labels, how='left', on='customer_ID')\n",
    "valid_complete = valid_final.merge(valid_labels, how='left', on='customer_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe0839",
   "metadata": {},
   "source": [
    "Next I define my DMatrices for train and validation using the dataframes from the previous step. I include all my features except the customer ID and the target variable as the training data, and I set the target as the label for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05d903d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = xgb.DMatrix(train_complete.drop(columns=['customer_ID', 'target']), label=train_complete.target)\n",
    "valid_matrix = xgb.DMatrix(valid_complete.drop(columns=['customer_ID', 'target']), label=valid_complete.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560af04",
   "metadata": {},
   "source": [
    "Now my matrices are created, I can move forward with defining the hyperparameters of the XGBoost model and training it on my data. The hyperparameters chosen for the initial pass are based on experimentation done in the xgboost-hyperparameter-tuning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be1c936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "seed = 42\n",
    "\n",
    "params = {\n",
    "    'verbosity': 1,\n",
    "    'max_depth': 4,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.15,\n",
    "    'random_state': seed,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41ada024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTrain-logloss:0.60497\tValid-logloss:0.60545\n",
      "[1]\tTrain-logloss:0.53857\tValid-logloss:0.53966\n",
      "[2]\tTrain-logloss:0.48732\tValid-logloss:0.48899\n",
      "[3]\tTrain-logloss:0.44650\tValid-logloss:0.44867\n",
      "[4]\tTrain-logloss:0.41350\tValid-logloss:0.41627\n",
      "[5]\tTrain-logloss:0.38661\tValid-logloss:0.39003\n",
      "[6]\tTrain-logloss:0.36436\tValid-logloss:0.36833\n",
      "[7]\tTrain-logloss:0.34600\tValid-logloss:0.35075\n",
      "[8]\tTrain-logloss:0.33049\tValid-logloss:0.33584\n",
      "[9]\tTrain-logloss:0.31737\tValid-logloss:0.32327\n",
      "[10]\tTrain-logloss:0.30575\tValid-logloss:0.31216\n",
      "[11]\tTrain-logloss:0.29597\tValid-logloss:0.30296\n",
      "[12]\tTrain-logloss:0.28773\tValid-logloss:0.29523\n",
      "[13]\tTrain-logloss:0.28026\tValid-logloss:0.28826\n",
      "[14]\tTrain-logloss:0.27385\tValid-logloss:0.28255\n",
      "[15]\tTrain-logloss:0.26794\tValid-logloss:0.27725\n",
      "[16]\tTrain-logloss:0.26297\tValid-logloss:0.27287\n",
      "[17]\tTrain-logloss:0.25843\tValid-logloss:0.26888\n",
      "[18]\tTrain-logloss:0.25460\tValid-logloss:0.26562\n",
      "[19]\tTrain-logloss:0.25104\tValid-logloss:0.26270\n",
      "[20]\tTrain-logloss:0.24797\tValid-logloss:0.26011\n",
      "[21]\tTrain-logloss:0.24490\tValid-logloss:0.25760\n",
      "[22]\tTrain-logloss:0.24242\tValid-logloss:0.25570\n",
      "[23]\tTrain-logloss:0.24004\tValid-logloss:0.25382\n",
      "[24]\tTrain-logloss:0.23760\tValid-logloss:0.25194\n",
      "[25]\tTrain-logloss:0.23545\tValid-logloss:0.25048\n",
      "[26]\tTrain-logloss:0.23366\tValid-logloss:0.24922\n",
      "[27]\tTrain-logloss:0.23185\tValid-logloss:0.24798\n",
      "[28]\tTrain-logloss:0.23026\tValid-logloss:0.24680\n",
      "[29]\tTrain-logloss:0.22865\tValid-logloss:0.24577\n",
      "[30]\tTrain-logloss:0.22713\tValid-logloss:0.24494\n",
      "[31]\tTrain-logloss:0.22562\tValid-logloss:0.24396\n",
      "[32]\tTrain-logloss:0.22426\tValid-logloss:0.24321\n",
      "[33]\tTrain-logloss:0.22318\tValid-logloss:0.24258\n",
      "[34]\tTrain-logloss:0.22191\tValid-logloss:0.24198\n",
      "[35]\tTrain-logloss:0.22074\tValid-logloss:0.24144\n",
      "[36]\tTrain-logloss:0.21961\tValid-logloss:0.24094\n",
      "[37]\tTrain-logloss:0.21841\tValid-logloss:0.24037\n",
      "[38]\tTrain-logloss:0.21740\tValid-logloss:0.23976\n",
      "[39]\tTrain-logloss:0.21642\tValid-logloss:0.23927\n",
      "[40]\tTrain-logloss:0.21556\tValid-logloss:0.23885\n",
      "[41]\tTrain-logloss:0.21458\tValid-logloss:0.23839\n",
      "[42]\tTrain-logloss:0.21370\tValid-logloss:0.23807\n",
      "[43]\tTrain-logloss:0.21276\tValid-logloss:0.23769\n",
      "[44]\tTrain-logloss:0.21178\tValid-logloss:0.23741\n",
      "[45]\tTrain-logloss:0.21099\tValid-logloss:0.23710\n",
      "[46]\tTrain-logloss:0.21003\tValid-logloss:0.23680\n",
      "[47]\tTrain-logloss:0.20926\tValid-logloss:0.23662\n",
      "[48]\tTrain-logloss:0.20835\tValid-logloss:0.23633\n",
      "[49]\tTrain-logloss:0.20756\tValid-logloss:0.23606\n",
      "[50]\tTrain-logloss:0.20667\tValid-logloss:0.23580\n",
      "[51]\tTrain-logloss:0.20584\tValid-logloss:0.23554\n",
      "[52]\tTrain-logloss:0.20508\tValid-logloss:0.23533\n",
      "[53]\tTrain-logloss:0.20444\tValid-logloss:0.23518\n",
      "[54]\tTrain-logloss:0.20374\tValid-logloss:0.23507\n",
      "[55]\tTrain-logloss:0.20286\tValid-logloss:0.23485\n",
      "[56]\tTrain-logloss:0.20200\tValid-logloss:0.23464\n",
      "[57]\tTrain-logloss:0.20116\tValid-logloss:0.23437\n",
      "[58]\tTrain-logloss:0.20061\tValid-logloss:0.23429\n",
      "[59]\tTrain-logloss:0.19994\tValid-logloss:0.23409\n",
      "[60]\tTrain-logloss:0.19934\tValid-logloss:0.23395\n",
      "[61]\tTrain-logloss:0.19881\tValid-logloss:0.23395\n",
      "[62]\tTrain-logloss:0.19802\tValid-logloss:0.23379\n",
      "[63]\tTrain-logloss:0.19734\tValid-logloss:0.23368\n",
      "[64]\tTrain-logloss:0.19670\tValid-logloss:0.23361\n",
      "[65]\tTrain-logloss:0.19595\tValid-logloss:0.23351\n",
      "[66]\tTrain-logloss:0.19547\tValid-logloss:0.23341\n",
      "[67]\tTrain-logloss:0.19494\tValid-logloss:0.23335\n",
      "[68]\tTrain-logloss:0.19438\tValid-logloss:0.23320\n",
      "[69]\tTrain-logloss:0.19388\tValid-logloss:0.23313\n",
      "[70]\tTrain-logloss:0.19324\tValid-logloss:0.23305\n",
      "[71]\tTrain-logloss:0.19251\tValid-logloss:0.23304\n",
      "[72]\tTrain-logloss:0.19188\tValid-logloss:0.23294\n",
      "[73]\tTrain-logloss:0.19147\tValid-logloss:0.23289\n",
      "[74]\tTrain-logloss:0.19082\tValid-logloss:0.23283\n",
      "[75]\tTrain-logloss:0.19014\tValid-logloss:0.23265\n",
      "[76]\tTrain-logloss:0.18942\tValid-logloss:0.23262\n",
      "[77]\tTrain-logloss:0.18884\tValid-logloss:0.23259\n",
      "[78]\tTrain-logloss:0.18844\tValid-logloss:0.23253\n",
      "[79]\tTrain-logloss:0.18800\tValid-logloss:0.23247\n",
      "[80]\tTrain-logloss:0.18763\tValid-logloss:0.23243\n",
      "[81]\tTrain-logloss:0.18691\tValid-logloss:0.23238\n",
      "[82]\tTrain-logloss:0.18633\tValid-logloss:0.23233\n",
      "[83]\tTrain-logloss:0.18567\tValid-logloss:0.23226\n",
      "[84]\tTrain-logloss:0.18537\tValid-logloss:0.23227\n",
      "[85]\tTrain-logloss:0.18472\tValid-logloss:0.23227\n",
      "[86]\tTrain-logloss:0.18435\tValid-logloss:0.23219\n",
      "[87]\tTrain-logloss:0.18402\tValid-logloss:0.23219\n",
      "[88]\tTrain-logloss:0.18333\tValid-logloss:0.23215\n",
      "[89]\tTrain-logloss:0.18274\tValid-logloss:0.23210\n",
      "[90]\tTrain-logloss:0.18218\tValid-logloss:0.23204\n",
      "[91]\tTrain-logloss:0.18152\tValid-logloss:0.23194\n",
      "[92]\tTrain-logloss:0.18084\tValid-logloss:0.23191\n",
      "[93]\tTrain-logloss:0.18050\tValid-logloss:0.23184\n",
      "[94]\tTrain-logloss:0.18016\tValid-logloss:0.23183\n",
      "[95]\tTrain-logloss:0.17959\tValid-logloss:0.23178\n",
      "[96]\tTrain-logloss:0.17918\tValid-logloss:0.23177\n",
      "[97]\tTrain-logloss:0.17876\tValid-logloss:0.23171\n",
      "[98]\tTrain-logloss:0.17816\tValid-logloss:0.23176\n",
      "[99]\tTrain-logloss:0.17748\tValid-logloss:0.23173\n",
      "[100]\tTrain-logloss:0.17690\tValid-logloss:0.23164\n",
      "[101]\tTrain-logloss:0.17652\tValid-logloss:0.23159\n",
      "[102]\tTrain-logloss:0.17627\tValid-logloss:0.23158\n",
      "[103]\tTrain-logloss:0.17575\tValid-logloss:0.23160\n",
      "[104]\tTrain-logloss:0.17519\tValid-logloss:0.23160\n",
      "[105]\tTrain-logloss:0.17472\tValid-logloss:0.23159\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(params, train_matrix, steps, early_stopping_rounds=3,\n",
    "                  evals=[(train_matrix, 'Train'), (valid_matrix, 'Valid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5c22b",
   "metadata": {},
   "source": [
    "The model achieves the lowest validation logloss to date. I bring in a function to calculate the amex_metric for my model (will be added to my script ASAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47ced221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluator(model, data, y_true):\n",
    "    \n",
    "    y_hat = model.predict(data)\n",
    "    \n",
    "    y_true_final = pd.DataFrame(y_true)\n",
    "    y_hat_final = pd.DataFrame(y_hat, columns=['prediction'])\n",
    "    \n",
    "    return myfunctions.amex_metric(y_true_final, y_hat_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66ebbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = model_evaluator(model, train_matrix, train_complete.target)\n",
    "valid_metric = model_evaluator(model, valid_matrix, valid_complete.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4056dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8763042974477815, 0.7708809200189399)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metric, valid_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02dff9",
   "metadata": {},
   "source": [
    "The model is overfit to my training data. It still performs well, considering the train dataset is roughly 1/10th the size of the validation dataset. I think I could greatly improve my results moving forward by training over the train + validation datasets combined and using the holdout set on Kaggle as the validation set. There are also opportunities here for hyperparameter tuning to improve the performance of my model.\n",
    "\n",
    "Key hyperparameters that are adjusted to reduce overfitting:  \n",
    "colsample_bytree  \n",
    "subsample   \n",
    "max_depth  \n",
    "gamma  \n",
    "eta  \n",
    "min_child_weight  \n",
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2566203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
