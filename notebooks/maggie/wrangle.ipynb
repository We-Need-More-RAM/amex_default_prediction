{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a966379c",
   "metadata": {},
   "source": [
    "Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a149860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas.core.window.ewm import ExponentialMovingWindow as emw\n",
    "\n",
    "import wrangle\n",
    "from wrangle import collapse_columns, get_null_count, get_zeros, get_delta_values, get_zeros_pct\n",
    "from wrangle import get_negative_count, get_ema, get_pctb, get_range, get_cv, get_negative_pct\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7122a68",
   "metadata": {},
   "source": [
    "Here we analyzed the feature importance based on the Gain metric from the best performing XGBoost model at this point. \n",
    "The goal is to identify the raw features that are providing the most information to the model as a way to reduce the number of features and thus the dimensionality. In doing that, we can focus on generating new features from the most influential and decrease performance time. We will take the top 20 features for a first run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb0b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/chunked/train_data_chunk0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c892c093e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/raw/train_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/chunked/train_data_chunk{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/chunked/train_data_chunk0.csv'"
     ]
    }
   ],
   "source": [
    "for i,chunk in enumerate(pd.read_csv('../../data/raw/train_data.csv', chunksize=350000)):\n",
    "    print(i)\n",
    "    chunk.to_csv('../../data/chunked/train_data_chunk{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408eabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a897bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "# X_df_header = pd.read_csv('../../data/raw/train_data.csv', nrows=0)\n",
    "X_df, y_df = wrangle.acquire_amex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d56bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cat_vars(X_df):\n",
    "    '''\n",
    "    this function will take the categorical variables and create a df with the dummy variables of those. \n",
    "    it returns that dataframe \n",
    "    '''\n",
    "    # create dataframe of categorical columns only\n",
    "    cat_columns = ['B_30', 'B_31', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    X_df_cat = X_df[['customer_ID'] + cat_columns]\n",
    "\n",
    "    # we will want to create dummy variables of categorical columns\n",
    "    X_df_cat = pd.get_dummies(X_df_cat, columns=cat_columns, drop_first=True)\n",
    "    return X_df_cat, cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7088b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_cat, cat_columns = prepare_cat_vars(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73f71d",
   "metadata": {},
   "source": [
    "Create new features out of numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_cols = ['S_2'] + cat_columns\n",
    "num_columns = [col for col in X_df.columns if col not in non_numeric_cols]\n",
    "X_df_num = X_df[num_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d44024",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals_df = get_null_count(X_df_num[['customer_ID'] + list(X_df_num.isnull().sum()[X_df_num.isnull().sum()>0].index)])\n",
    "missing_vals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e34169",
   "metadata": {},
   "source": [
    "Take care of all nulls and outliers in the numeric columns:\n",
    "\n",
    "1. for values < -1, set value to -1. \n",
    "2. for values > 10, set value to 10. \n",
    "3. for variables where the min is > 0, set nulls to 0. \n",
    "4. for variables where the min is <= 0, set nulls to -2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ec1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_df_num.drop(columns=['customer_ID']).columns:\n",
    "    # for values < -1, set to -1\n",
    "    X_df_num.loc[X_df_num[col] < -1, col] = -1\n",
    "    # for values > 10, set to 10\n",
    "    X_df_num.loc[X_df_num[col] > 10, col] = 10\n",
    "\n",
    "# create a descriptive stats dataframe I will reference in the next step\n",
    "df_stats = X_df_num.drop(columns=['customer_ID']).describe().T\n",
    "\n",
    "# find the name of variables where the min is > 0\n",
    "fill_with_zero = df_stats[df_stats['min'] > 0].index\n",
    "# find the name of variables where the min is <= 0\n",
    "fill_with_neg2 = df_stats[df_stats['min'] <= 0].index\n",
    "\n",
    "# for those vars where min > 0, set nulls to 0\n",
    "for col in fill_with_zero:\n",
    "    X_df_num.loc[X_df_num[col].isnull(), col] = 0\n",
    "\n",
    "# for those vars where min <= 0, set nulls to -2 (the min will not be < -1)\n",
    "for col in fill_with_neg2:\n",
    "    X_df_num.loc[X_df_num[col].isnull(), col] = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e614022",
   "metadata": {},
   "source": [
    "So X_df_cat, X_df_num, and missing_vals_df are all free of nulls now. \n",
    "\n",
    "Next, get number of negative values for each column, as that seems to be an indicator for many variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with the number of records with a negative value for each variable for each customer. \n",
    "neg_df = get_negative_count(X_df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with the number of records of 0 value of each variable for each customer. \n",
    "zero_df = get_zeros(X_df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e193cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the percent of records that have a value of 0 for each variable for each customer. \n",
    "zero_pct_df = get_zeros_pct(X_df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf48552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the percent of records that have a negative value for each variable for each customer. \n",
    "neg_pct_df = get_negative_pct(X_df_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322728d",
   "metadata": {},
   "source": [
    "Now, I will concatenate the X_df_cat and X_df_num so that we can then aggregate by grouping by customer_ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274005cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.concat([X_df_cat, X_df_num.drop(columns=['customer_ID'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c300b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714754bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with the last value, standard deviation, min and max of each variable for each customer. \n",
    "agg_df = X_df.groupby('customer_ID').agg(['last', 'median', 'mean', 'std', 'min', 'max'])\n",
    "agg_df = collapse_columns(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with the difference between the last value and the value 2 months prior for each variable for each customer. \n",
    "delta_df = get_delta_values(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with the exponential moving average, with a weight of .8, for each variable for each customer. \n",
    "ema_df = get_ema(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all of the above dataframes into a single 'metrics' dataframe\n",
    "metrics_df = pd.concat([agg_df, missing_vals_df, zero_df, zero_pct_df, neg_pct_df, neg_df, delta_df, ema_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4bb9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature, pctb, which is the percent b value for each variable for each customer. \n",
    "metrics_df = get_pctb(X_df, metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ab171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature, range, which is the max - min for each variable for each customer. \n",
    "metrics_df = get_range(X_df, metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07188eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature, cv, which represents the coefficient of variation (std/mean) for each variable for each customer. \n",
    "metrics_df = get_cv(X_df, metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4780ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the _min and _std columns. The info for these is captured in _range and _cv\n",
    "cols_to_drop = metrics_df.filter(regex='(_std)$', axis=1).columns\n",
    "metrics_df = metrics_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep all columns for now\n",
    "cols_to_keep = num_columns[1:] + list(X_df_cat.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d139a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_keep:\n",
    "    # where ema is null, replace it with the last value\n",
    "    metrics_df.loc[metrics_df[col+'_ema'].isnull(), col+'_ema'] = metrics_df.loc[:,col+'_last']\n",
    "    # where pct b is null, replace it with .5, the midpoint\n",
    "    metrics_df.loc[metrics_df[col+'_pctb'].isnull(), col+'_pctb'] = .5\n",
    "    # fill null cv records with 0\n",
    "    metrics_df[col+'_cv'].fillna(value=0, inplace=True)\n",
    "    # fill null values of difference with 0, this happens when there are not at least 2 months. \n",
    "    metrics_df[col+'_diff'].fillna(value=0, inplace=True)\n",
    "    # fill null diff_mean values with 0\n",
    "    metrics_df[col+'_diff_mean'].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d084dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those columns where > 90% of rows are missing values\n",
    "# missing_counts_df = pd.DataFrame({'missing_count': metrics_df.isnull().sum(), 'missing_pct': metrics_df.isnull().sum()/len(metrics_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d226ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_drop = missing_counts_df[missing_counts_df.missing_pct > .90].index\n",
    "# features_df = metrics_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fcc26",
   "metadata": {},
   "source": [
    "Missing values that are returned after the creation of new features are due to the following:\n",
    "\n",
    "1. _diff, _ema, _cv, _%b when the customer only has one month of data. \n",
    "2. _diff_mean when the customer only has two months of data. \n",
    "\n",
    "I will replace missing values with the following: \n",
    "\n",
    "1. Fill cv with 0\n",
    "2. Fill %b with .5 because that is the value when the last value is equal to the mean. \n",
    "3. Fill diff with 0\n",
    "4. Fill diff_mean with 0\n",
    "5. Fill ema with last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values complete!\n",
    "metrics_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.rename(columns={'index': 'customer_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = wrangle.split_amex(metrics_df, \n",
    "                                                                              y_df, \n",
    "                                                                              train_size=.5, \n",
    "                                                                              test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ba212",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(y_train, how='left', on='customer_ID')\n",
    "validate = X_validate.merge(y_validate, how='left', on='customer_ID')\n",
    "test = X_test.merge(y_test, how='left', on='customer_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    print(col)\n",
    "    print(train[col].describe()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e46c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.drop(columns=['customer_ID']).columns:\n",
    "    # fill inf values of _cv \n",
    "    train.loc[np.isinf(np.array(train[col])), col] = 0\n",
    "    validate.loc[np.isinf(np.array(validate[col])), col] = 0\n",
    "    test.loc[np.isinf(np.array(test[col])), col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = xgb.DMatrix(train.drop(columns=['customer_ID', 'target']), label=train.target)\n",
    "valid_matrix = xgb.DMatrix(validate.drop(columns=['customer_ID', 'target']), label=validate.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2879ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "seed = 42\n",
    "\n",
    "params = {\n",
    "    'verbosity': 1,\n",
    "    'max_depth': 3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.075,\n",
    "    'random_state': seed,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.8, \n",
    "    'subsample': 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df46005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, train_matrix, steps, early_stopping_rounds=10,\n",
    "                  evals=[(train_matrix, 'Train'), (valid_matrix, 'Valid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7670f",
   "metadata": {},
   "source": [
    "Run line by line, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a09cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv('features_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d12f43",
   "metadata": {},
   "source": [
    "Flatten the time series data. \n",
    "\n",
    "For each variable, we need to create the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a10f4",
   "metadata": {},
   "source": [
    "Explore the different columns, datatypes, descriptive stats\n",
    "\n",
    "For reference: \n",
    "* D_* = Delinquency variables\n",
    "* S_* = Spend variables\n",
    "* P_* = Payment variables\n",
    "* B_* = Balance variables\n",
    "* R_* = Risk variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend = X_df.iloc[:,X_df.columns.str[0] == 'S']\n",
    "delinq = X_df.iloc[:,X_df.columns.str[0] == 'D']\n",
    "pay = X_df.iloc[:,X_df.columns.str[0] == 'P']\n",
    "balance = X_df.iloc[:,X_df.columns.str[0] == 'B']\n",
    "risk = X_df.iloc[:,X_df.columns.str[0] == 'R']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8176f",
   "metadata": {},
   "source": [
    "**Spend variables**\n",
    "\n",
    "- 22 total columns\n",
    "\n",
    "- S_2: date *needs to be converted* **done**\n",
    "\n",
    "- All others: float\n",
    "\n",
    "- S_2, S_5, S_6, S_8, S_11:S_13, S_15:S_20 : no missing values\n",
    "\n",
    "- S_22:S_26 : missing < 1% of values\n",
    "\n",
    "- S_3, S_7, S_27 : missing 1-25% of values\n",
    "\n",
    "- S_9, S_27 : missing 25-75% of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f71f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93641bca",
   "metadata": {},
   "source": [
    "**Delinquency Variables**\n",
    "\n",
    "- 96 total columns\n",
    "\n",
    "- D_63: Object\n",
    "\n",
    "- D_64: Object\n",
    "\n",
    "- All others: float\n",
    "\n",
    "- D_39, D_47, D_51, D_58, D_60, D_63, D_65, D_71, D_75, D_86, D_92, D_93, D_94, D_96, D_127 : no missing values\n",
    "\n",
    "- D_42, D_49, D_66, D_73, D_76, D_87, D_88, D_106, D_108, D_110, D_111, D_132, D_134:D_138, D_142 : missing > 75% of values.\n",
    "\n",
    "- D_41, D_44:D_46, D_48, D_52, D_54:D_55, D_59, D_61, D_62, D_64, D_68:D_70, D_72, D_74, D_78:D_81, D_83, D_84, D_89, D_91, D_102:D_104, D_107, D_109, D_112:D_126, D_128:D_131, D_133, D_139:D_145: missing < 25%\n",
    "\n",
    "- D_43, D_50, D_53 D_56, D_77, D_82, D_105 : 25-75% missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq.D_63.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f930a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq.D_64.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955709d",
   "metadata": {},
   "source": [
    "**Payment Variables**\n",
    "\n",
    "- 3 total columns (P_2, P_3, P_4)\n",
    "\n",
    "- all: float\n",
    "\n",
    "- P_4 : no missing values\n",
    "\n",
    "- P_2 & P_3 : missing < 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d286d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7154901",
   "metadata": {},
   "source": [
    "**Balance Variables**\n",
    "\n",
    "- 40 variables\n",
    "\n",
    "- B_31: int (0, 1)\n",
    "\n",
    "- all others: float\n",
    "\n",
    "- B_29, B_39, and B_42 are majority null\n",
    "\n",
    "- B_17 is missing \n",
    "\n",
    "- B_1, B_4, B_5, B_7, B_9, B_10, B_11, B_12, B_14, B_18, B_21, B_23, B_24, B_28, B_31, B_32, B_36 have no missing values. \n",
    "\n",
    "- B_2, B_3, B_6, B_8, B_13, B_15, B_16, B_19, B_20, B_25, B_26, B_27, B_30, B_33, B_37, B_38, B_40, B_41 are missing < 1% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.B_31.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7db81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe08257",
   "metadata": {},
   "source": [
    "**Risk Variables**\n",
    "\n",
    "- 28 Columns\n",
    "\n",
    "- All: float\n",
    "\n",
    "- R_9, R_26: missing > 90% of values. \n",
    "\n",
    "- R_12, R_20, and R_27 are missing < 1%\n",
    "\n",
    "- R_1:R_8, R_10:R_11, R13:R19, R21:R26, R28 :  no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa46f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lists of column names by datatype for future use in analysis\n",
    "object_cols = ['D_63', 'D_64']\n",
    "int_cols = ['B_31']\n",
    "date_cols = ['S_2']\n",
    "\n",
    "# list of non_float columns in order to generate a list of all float column names (186 columns)\n",
    "non_float_cols = object_cols + int_cols + date_cols\n",
    "float_cols = [col for col in X_df.columns if col not in non_float_cols]\n",
    "len(float_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None,):\n",
    "    print(null_df.sort_values('total_nulls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.groupby('feature_category').percent_nulls.agg(['mean', 'median', 'max', 'min']).sort_values('mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.target.value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
