{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a966379c",
   "metadata": {},
   "source": [
    "Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a149860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas.core.window.ewm import ExponentialMovingWindow as emw\n",
    "\n",
    "import wrangle\n",
    "from wrangle import collapse_columns, get_null_count, get_zeros, get_delta_values, get_ema, get_pctb, get_range, get_cv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcc4a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "# X_df_header = pd.read_csv('../../data/raw/train_data.csv', nrows=0)\n",
    "X_df = pd.read_csv('../../data/raw/train_data.csv', nrows=19995) #, skiprows=0,names=X_df_header.columns)\n",
    "X_df = X_df.drop(columns=['S_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25992a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of categorical columns only\n",
    "cat_columns = ['B_30', 'B_31', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "X_df_cat = X_df[['customer_ID'] + cat_columns]\n",
    "\n",
    "# we will want to create dummy variables of categorical columns\n",
    "X_df_cat = pd.get_dummies(X_df_cat, columns=cat_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506b025",
   "metadata": {},
   "source": [
    "Explore categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da1487dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B_30_1.0     0.150038\n",
       "B_30_2.0     0.011103\n",
       "B_31_1       0.997249\n",
       "B_38_2.0     0.331383\n",
       "B_38_3.0     0.237359\n",
       "B_38_4.0     0.058465\n",
       "B_38_5.0     0.080520\n",
       "B_38_6.0     0.028357\n",
       "B_38_7.0     0.055864\n",
       "D_114_1.0    0.612453\n",
       "D_116_1.0    0.000950\n",
       "D_117_1.0    0.024756\n",
       "D_117_2.0    0.128632\n",
       "D_117_3.0    0.202851\n",
       "D_117_4.0    0.209702\n",
       "D_117_5.0    0.087872\n",
       "D_117_6.0    0.069167\n",
       "D_120_1.0    0.111078\n",
       "D_126_0.0    0.163541\n",
       "D_126_1.0    0.771593\n",
       "D_63_CO      0.742336\n",
       "D_63_CR      0.160490\n",
       "D_63_XL      0.001650\n",
       "D_63_XM      0.001800\n",
       "D_63_XZ      0.005401\n",
       "D_64_O       0.556189\n",
       "D_64_R       0.137334\n",
       "D_64_U       0.264066\n",
       "D_66_1.0     0.108127\n",
       "D_68_1.0     0.019405\n",
       "D_68_2.0     0.036409\n",
       "D_68_3.0     0.085921\n",
       "D_68_4.0     0.085871\n",
       "D_68_5.0     0.214754\n",
       "D_68_6.0     0.520030\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df_cat.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e86841",
   "metadata": {},
   "source": [
    "Create new features out of numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b9e07fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = [col for col in X_df.columns if col[0] not in cat_columns]\n",
    "X_df_num = X_df[num_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "197ecd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = wrangle.clean_amex(X_df).drop(columns=['S_2'])\n",
    "\n",
    "# we will want to create dummy variables of categorical columns\n",
    "X_df = pd.get_dummies(X_df, columns=cat_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f18fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = wrangle.get_features(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f98daa6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['B_30', 'B_31', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126',\\n       'D_63', 'D_64', 'D_66', 'D_68'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-fbbf2544bae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcat_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'B_30'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B_31'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B_38'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_114'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_116'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_117'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_126'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_63'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_66'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_68'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# num_columns =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['B_30', 'B_31', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126',\\n       'D_63', 'D_64', 'D_66', 'D_68'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "# num_columns = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1fb4438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_ID',\n",
       " 'P_2',\n",
       " 'D_39',\n",
       " 'B_1',\n",
       " 'B_2',\n",
       " 'R_1',\n",
       " 'S_3',\n",
       " 'D_41',\n",
       " 'B_3',\n",
       " 'D_42',\n",
       " 'D_43',\n",
       " 'D_44',\n",
       " 'B_4',\n",
       " 'D_45',\n",
       " 'B_5',\n",
       " 'R_2',\n",
       " 'D_46',\n",
       " 'D_47',\n",
       " 'D_48',\n",
       " 'D_49',\n",
       " 'B_6',\n",
       " 'B_7',\n",
       " 'B_8',\n",
       " 'D_50',\n",
       " 'D_51',\n",
       " 'B_9',\n",
       " 'R_3',\n",
       " 'D_52',\n",
       " 'P_3',\n",
       " 'B_10',\n",
       " 'D_53',\n",
       " 'S_5',\n",
       " 'B_11',\n",
       " 'S_6',\n",
       " 'D_54',\n",
       " 'R_4',\n",
       " 'S_7',\n",
       " 'B_12',\n",
       " 'S_8',\n",
       " 'D_55',\n",
       " 'D_56',\n",
       " 'B_13',\n",
       " 'R_5',\n",
       " 'D_58',\n",
       " 'S_9',\n",
       " 'B_14',\n",
       " 'D_59',\n",
       " 'D_60',\n",
       " 'D_61',\n",
       " 'B_15',\n",
       " 'S_11',\n",
       " 'D_62',\n",
       " 'D_65',\n",
       " 'B_16',\n",
       " 'B_17',\n",
       " 'B_18',\n",
       " 'B_19',\n",
       " 'B_20',\n",
       " 'S_12',\n",
       " 'R_6',\n",
       " 'S_13',\n",
       " 'B_21',\n",
       " 'D_69',\n",
       " 'B_22',\n",
       " 'D_70',\n",
       " 'D_71',\n",
       " 'D_72',\n",
       " 'S_15',\n",
       " 'B_23',\n",
       " 'D_73',\n",
       " 'P_4',\n",
       " 'D_74',\n",
       " 'D_75',\n",
       " 'D_76',\n",
       " 'B_24',\n",
       " 'R_7',\n",
       " 'D_77',\n",
       " 'B_25',\n",
       " 'B_26',\n",
       " 'D_78',\n",
       " 'D_79',\n",
       " 'R_8',\n",
       " 'R_9',\n",
       " 'S_16',\n",
       " 'D_80',\n",
       " 'R_10',\n",
       " 'R_11',\n",
       " 'B_27',\n",
       " 'D_81',\n",
       " 'D_82',\n",
       " 'S_17',\n",
       " 'R_12',\n",
       " 'B_28',\n",
       " 'R_13',\n",
       " 'D_83',\n",
       " 'R_14',\n",
       " 'R_15',\n",
       " 'D_84',\n",
       " 'R_16',\n",
       " 'B_29',\n",
       " 'S_18',\n",
       " 'D_86',\n",
       " 'D_87',\n",
       " 'R_17',\n",
       " 'R_18',\n",
       " 'D_88',\n",
       " 'S_19',\n",
       " 'R_19',\n",
       " 'B_32',\n",
       " 'S_20',\n",
       " 'R_20',\n",
       " 'R_21',\n",
       " 'B_33',\n",
       " 'D_89',\n",
       " 'R_22',\n",
       " 'R_23',\n",
       " 'D_91',\n",
       " 'D_92',\n",
       " 'D_93',\n",
       " 'D_94',\n",
       " 'R_24',\n",
       " 'R_25',\n",
       " 'D_96',\n",
       " 'S_22',\n",
       " 'S_23',\n",
       " 'S_24',\n",
       " 'S_25',\n",
       " 'S_26',\n",
       " 'D_102',\n",
       " 'D_103',\n",
       " 'D_104',\n",
       " 'D_105',\n",
       " 'D_106',\n",
       " 'D_107',\n",
       " 'B_36',\n",
       " 'B_37',\n",
       " 'R_26',\n",
       " 'R_27',\n",
       " 'D_108',\n",
       " 'D_109',\n",
       " 'D_110',\n",
       " 'D_111',\n",
       " 'B_39',\n",
       " 'D_112',\n",
       " 'B_40',\n",
       " 'S_27',\n",
       " 'D_113',\n",
       " 'D_115',\n",
       " 'D_118',\n",
       " 'D_119',\n",
       " 'D_121',\n",
       " 'D_122',\n",
       " 'D_123',\n",
       " 'D_124',\n",
       " 'D_125',\n",
       " 'D_127',\n",
       " 'D_128',\n",
       " 'D_129',\n",
       " 'B_41',\n",
       " 'B_42',\n",
       " 'D_130',\n",
       " 'D_131',\n",
       " 'D_132',\n",
       " 'D_133',\n",
       " 'R_28',\n",
       " 'D_134',\n",
       " 'D_135',\n",
       " 'D_136',\n",
       " 'D_137',\n",
       " 'D_138',\n",
       " 'D_139',\n",
       " 'D_140',\n",
       " 'D_141',\n",
       " 'D_142',\n",
       " 'D_143',\n",
       " 'D_144',\n",
       " 'D_145',\n",
       " 'B_30_1.0',\n",
       " 'B_30_2.0',\n",
       " 'B_31_1',\n",
       " 'B_38_2.0',\n",
       " 'B_38_3.0',\n",
       " 'B_38_4.0',\n",
       " 'B_38_5.0',\n",
       " 'B_38_6.0',\n",
       " 'B_38_7.0',\n",
       " 'D_114_1.0',\n",
       " 'D_116_1.0',\n",
       " 'D_117_1.0',\n",
       " 'D_117_2.0',\n",
       " 'D_117_3.0',\n",
       " 'D_117_4.0',\n",
       " 'D_117_5.0',\n",
       " 'D_117_6.0',\n",
       " 'D_120_1.0',\n",
       " 'D_126_0.0',\n",
       " 'D_126_1.0',\n",
       " 'D_63_CO',\n",
       " 'D_63_CR',\n",
       " 'D_63_XL',\n",
       " 'D_63_XM',\n",
       " 'D_63_XZ',\n",
       " 'D_64_O',\n",
       " 'D_64_R',\n",
       " 'D_64_U',\n",
       " 'D_66_1.0',\n",
       " 'D_68_1.0',\n",
       " 'D_68_2.0',\n",
       " 'D_68_3.0',\n",
       " 'D_68_4.0',\n",
       " 'D_68_5.0',\n",
       " 'D_68_6.0']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# result = [x for x in list_a if x[0] in list_b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714754bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = X_df.groupby('customer_ID').agg(['last', 'std', 'min', 'max'])\n",
    "agg_df = collapse_columns(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c34fcf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B_30_1.0_last     0\n",
       "B_30_1.0_std     22\n",
       "B_30_1.0_min      0\n",
       "B_30_1.0_max      0\n",
       "B_30_2.0_last     0\n",
       "B_30_2.0_std     22\n",
       "B_30_2.0_min      0\n",
       "B_30_2.0_max      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.filter(regex='B_30').isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d226ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "    missing_vals_df = get_null_count(X_df)\n",
    "    zero_df = get_zeros(X_df)\n",
    "    delta_df = get_delta_values(X_df)\n",
    "    ema_df = get_ema(X_df)\n",
    "\n",
    "    metrics_df = pd.concat([agg_df, missing_vals_df, zero_df, delta_df, ema_df],axis=1)\n",
    "\n",
    "    metrics_df = get_pctb(X_df, metrics_df)\n",
    "    metrics_df = get_range(X_df, metrics_df)\n",
    "    metrics_df = get_cv(X_df, metrics_df)\n",
    "\n",
    "    # drop the _min and _std columns. Those are captured in _range and _cv\n",
    "    cols_to_drop = metrics_df.filter(regex='(_min|_std)$', axis=1).columns\n",
    "    metrics_df = metrics_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # drop those columns where > 90% of rows are missing values\n",
    "    missing_counts_df = pd.DataFrame({'missing_count': metrics_df.isnull().sum(), 'missing_pct': metrics_df.isnull().sum()/len(metrics_df)})\n",
    "    cols_to_drop = missing_counts_df[missing_counts_df.missing_pct > .90].index\n",
    "    features_df = metrics_df.drop(columns=cols_to_drop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71bbbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent(data):\n",
    "    \"\"\"Calculates entropy of the passed `pd.Series`\n",
    "    \"\"\"\n",
    "    p_data = data.value_counts()           # counts occurrence of each value\n",
    "    entropy = scipy.stats.entropy(p_data)  # get entropy from counts\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def compute_entropy(features_df, ent):\n",
    "    entropy_series = features_df.apply(ent)\n",
    "    features_df = features_df[entropy_series[entropy_series > 1].index]\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a009d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = compute_entropy(features_df, ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "908460be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1663"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "len(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94ddf874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B_30_1.0_cv       1201\n",
       "B_30_2.0_cv       1484\n",
       "B_30_1.0_%b          0\n",
       "B_30_1.0_%b       1273\n",
       "B_30_2.0_%b          0\n",
       "B_30_2.0_%b       1484\n",
       "B_30_1.0_zeros       0\n",
       "B_30_2.0_zeros       0\n",
       "B_30_1.0_ema        22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c14db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('features.csv')\n",
    "df2 = pd.read_csv('features_2.csv')\n",
    "df = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59887efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = pd.DataFrame((df.isnull().sum()/len(df)), columns=['pct_missing'])[1:]\n",
    "missing_counts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b30f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['D_106_diff_mean'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[numerical_cols].clip(lower=-3, upper=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Feature Engineering functions #\n",
    "#################################\n",
    "\n",
    "def collapse_columns(X_df):\n",
    "    '''\n",
    "    this function will collapse the multi-level index of the columns \n",
    "    that are generated after computing the first set of aggregates in \n",
    "    our groupby function in the agg_features function.\n",
    "    '''\n",
    "    # df = X_df.copy()\n",
    "    if isinstance(X_df.columns, pd.MultiIndex):\n",
    "        X_df.columns = X_df.columns.to_series().apply(lambda x: \"_\".join(x))\n",
    "    return X_df\n",
    "\n",
    "def get_null_count(X_df):\n",
    "    '''\n",
    "    this function will calculate the number of missing values for each feature. \n",
    "    it reaturns a dataframe with the columns: <column_name_orig>_nulls \n",
    "    '''\n",
    "    missing_vals = X_df.groupby('customer_ID').agg(lambda x: x.isnull().sum())\n",
    "    missing_vals.columns = [x + '_nulls' for x in missing_vals.columns]\n",
    "    return missing_vals\n",
    "\n",
    "def get_zeros(X_df):\n",
    "    '''\n",
    "    this function will calculate the number of zeros values for each feature. \n",
    "    it reaturns a dataframe with the columns: <column_name_orig>_zeros \n",
    "    '''\n",
    "    zeros_df = X_df.groupby('customer_ID').agg(lambda x: (x == 0.0).sum())\n",
    "    zeros_df.columns = [x + '_zeros' for x in zeros_df.columns]\n",
    "    return zeros_df\n",
    "\n",
    "# def get_cv(X_df):\n",
    "#     '''\n",
    "#     this function will compute the coefficient of variation for each feature. \n",
    "#     it reaturns a dataframe with the columns: <column_name_orig>_cv \n",
    "#     '''\n",
    "#     cv_df = X_df.groupby('customer_ID').agg(lambda x: x.std()/x.mean())\n",
    "#     cv_df.columns = [x + '_cv' for x in cv_df.columns]\n",
    "#     return cv_df\n",
    "\n",
    "def get_two_period_difference(X_df):\n",
    "    '''\n",
    "    This function computes the 2-period in values for each feature. \n",
    "    it returns a dataframe with the customer id set to the index. \n",
    "    the function is used in compute_delta_values() function\n",
    "    '''\n",
    "    delta_df = X_df.groupby('customer_ID').diff(periods=2)\n",
    "    delta_df.index = X_df.customer_ID\n",
    "    return delta_df\n",
    "\n",
    "    \n",
    "def get_delta_values(X_df):\n",
    "    '''\n",
    "    This function first gets the two-period difference in values for each feature and assigns that to a dataframe (delta).\n",
    "    It generates a dataframe of the most recent 2-period difference (delta_value).\n",
    "    Next, from the delta dataframe, it computes the number of negative deltas over the customer's history and \n",
    "    assigns that to a dataframe (neg_delta_count).\n",
    "    Next, it uses the delta dataframe to compute the average delta over the customer's history and assigns that to \n",
    "    a dadtaframe (delta_mean).\n",
    "    Finally, all of these dataframes are concatenated into a single dataframe, delta_df. \n",
    "    '''\n",
    "    # first compute the 2 period delta and create a dataframe with those values\n",
    "    delta_df = get_two_period_difference(X_df)\n",
    "    delta_df.columns = [x + '_diff' for x in delta_df.columns]\n",
    "    \n",
    "    # Use the delta df to take the last value as the current delta\n",
    "    delta_value = delta_df.groupby('customer_ID').last()\n",
    "    \n",
    "    # use the delta df to count the number of changes over customer history that were negative\n",
    "    neg_delta_count = delta_df.groupby('customer_ID').agg(lambda x: (x < 0).sum())\n",
    "    neg_delta_count.columns = [x + '_count' for x in delta_df.columns]\n",
    "    \n",
    "    # use the delta df to compute the rolling average of the delta values\n",
    "    delta_mean = delta_df.groupby('customer_ID').transform(lambda x: x.rolling(window=6, \n",
    "                                                                       min_periods=3, \n",
    "                                                                       closed='left').mean())\n",
    "    delta_mean.columns = [x + '_mean' for x in delta_df.columns]\n",
    "    \n",
    "    # take the last value, the current average of change\n",
    "    delta_mean = delta_mean.groupby('customer_ID').last()\n",
    "    \n",
    "    # concatenate the dataframes with the computed values by concatenating columns along the customer index\n",
    "    delta_df = pd.concat([delta_value, neg_delta_count, delta_mean], axis=1)\n",
    "    return delta_df\n",
    "\n",
    "def get_ema(X_df):\n",
    "    '''\n",
    "    This function will compute the exponential moving average, with an alpha of .8. \n",
    "    it returns a dataframe with the columns: <column_name_orig>_ema. \n",
    "    '''\n",
    "    ema_df = X_df.groupby('customer_ID').transform(lambda x: x.ewm(alpha=.8, min_periods=1, adjust=True).mean().shift(periods=1))\n",
    "    ema_df.columns = [x + '_ema' for x in ema_df.columns]\n",
    "    ema_df.index = X_df.customer_ID\n",
    "    ema_df = ema_df.groupby('customer_ID').last()\n",
    "    return ema_df\n",
    "\n",
    "def get_pctb(X_df, metrics_df):\n",
    "    df_customer_indexed = X_df.set_index('customer_ID')\n",
    "    pctb_series = pd.Series()\n",
    "\n",
    "    # loop through original column names and for eacsh one, compute pctb\n",
    "    k = 6\n",
    "    for x in df_customer_indexed.columns:\n",
    "        ubb = metrics_df[x + '_ema'] + k*metrics_df[x + '_std']\n",
    "        lbb = metrics_df[(x + '_ema')] - k*metrics_df[x + '_std']\n",
    "        pctb = (metrics_df[x + '_last'] - lbb) / (ubb - lbb)\n",
    "        pctb_series = pd.concat([pctb_series, pctb], axis=1)\n",
    "    \n",
    "    pctb_df = pd.DataFrame(pctb_series)\n",
    "    pctb_df = pctb_df.iloc[:,1:]\n",
    "    pctb_df.columns = [x + '_%b' for x in df_customer_indexed.columns]\n",
    "    metrics_df = pd.concat([pctb_df, metrics_df], axis=1)\n",
    "    return metrics_df\n",
    "\n",
    "def get_range(X_df, metrics_df):\n",
    "    range_series = pd.Series()\n",
    "    for x in df_customer_indexed.columns:\n",
    "        range_val = metrics_df[x + '_max'] - metrics_df[x + '_min']\n",
    "        range_series = pd.concat([range_series, range_val], axis=1)\n",
    "\n",
    "    range_df = pd.DataFrame(range_series)\n",
    "    range_df = range_df.iloc[:,1:]\n",
    "    range_df.columns = [x + '_%b' for x in df_customer_indexed.columns]\n",
    "    metrics_df = pd.concat([range_df, metrics_df], axis=1)\n",
    "    return metrics_df\n",
    "\n",
    "def get_cv(X_df, metrics_df):\n",
    "    cv_series = pd.Series()\n",
    "    for x in df_customer_indexed.columns:\n",
    "        cv = metrics_df[x + '_std']/metrics_df[x + '_ema']\n",
    "        cv_series = pd.concat([cv_series, cv], axis=1)\n",
    "\n",
    "    cv_df = pd.DataFrame(cv_series)\n",
    "    cv_df = cv_df.iloc[:,1:]\n",
    "    cv_df.columns = [x + '_cv' for x in df_customer_indexed.columns]\n",
    "    metrics_df = pd.concat([cv_df, metrics_df], axis=1)\n",
    "    return metrics_df\n",
    "\n",
    "def ent(data):\n",
    "    \"\"\"Calculates entropy of the passed `pd.Series`\n",
    "    \"\"\"\n",
    "    p_data = data.value_counts()           # counts occurrence of each value\n",
    "    entropy = scipy.stats.entropy(p_data)  # get entropy from counts\n",
    "    return entropy\n",
    "\n",
    "def get_features(X_df):\n",
    "\n",
    "    agg_df = X_df.groupby('customer_ID').agg(['last', 'std', 'min', 'max'])\n",
    "    agg_df = collapse_columns(agg_df)\n",
    "\n",
    "    missing_vals_df = get_null_count(X_df)\n",
    "    zero_df = get_zeros(X_df)\n",
    "    delta_df = get_delta_values(X_df)\n",
    "    ema_df = get_ema(X_df)\n",
    "\n",
    "    metrics_df = pd.concat([agg_df, missing_vals_df, zero_df, delta_df, ema_df],axis=1)\n",
    "\n",
    "    metrics_df = get_pctb(X_df, metrics_df)\n",
    "    metrics_df = get_range(X_df, metrics_df)\n",
    "    metrics_df = get_cv(X_df, metrics_df)\n",
    "\n",
    "    # drop the _min and _std columns. Those are captured in _range and _cv\n",
    "    cols_to_drop = metrics_df.filter(regex='(_min|_std)$', axis=1).columns\n",
    "    metrics_df = metrics_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # drop those columns where > 90% of rows are missing values\n",
    "    missing_counts_df = pd.DataFrame({'missing_count': metrics_df.isnull().sum(), 'missing_pct': metrics_df.isnull().sum()/len(metrics_df)})\n",
    "    cols_to_drop = missing_counts_df[missing_counts_df.missing_pct > .90].index\n",
    "    metrics_df = metrics_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    entropy_series = metrics_df.apply(ent)\n",
    "    features_df = metrics_df[entropy_series[entropy_series > 1].index]\n",
    "    return features_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7670f",
   "metadata": {},
   "source": [
    "Run line by line, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a09cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv('features_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d12f43",
   "metadata": {},
   "source": [
    "Flatten the time series data. \n",
    "\n",
    "For each variable, we need to create the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a10f4",
   "metadata": {},
   "source": [
    "Explore the different columns, datatypes, descriptive stats\n",
    "\n",
    "For reference: \n",
    "* D_* = Delinquency variables\n",
    "* S_* = Spend variables\n",
    "* P_* = Payment variables\n",
    "* B_* = Balance variables\n",
    "* R_* = Risk variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend = X_df.iloc[:,X_df.columns.str[0] == 'S']\n",
    "delinq = X_df.iloc[:,X_df.columns.str[0] == 'D']\n",
    "pay = X_df.iloc[:,X_df.columns.str[0] == 'P']\n",
    "balance = X_df.iloc[:,X_df.columns.str[0] == 'B']\n",
    "risk = X_df.iloc[:,X_df.columns.str[0] == 'R']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8176f",
   "metadata": {},
   "source": [
    "**Spend variables**\n",
    "\n",
    "- 22 total columns\n",
    "\n",
    "- S_2: date *needs to be converted* **done**\n",
    "\n",
    "- All others: float\n",
    "\n",
    "- S_2, S_5, S_6, S_8, S_11:S_13, S_15:S_20 : no missing values\n",
    "\n",
    "- S_22:S_26 : missing < 1% of values\n",
    "\n",
    "- S_3, S_7, S_27 : missing 1-25% of values\n",
    "\n",
    "- S_9, S_27 : missing 25-75% of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f71f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93641bca",
   "metadata": {},
   "source": [
    "**Delinquency Variables**\n",
    "\n",
    "- 96 total columns\n",
    "\n",
    "- D_63: Object\n",
    "\n",
    "- D_64: Object\n",
    "\n",
    "- All others: float\n",
    "\n",
    "- D_39, D_47, D_51, D_58, D_60, D_63, D_65, D_71, D_75, D_86, D_92, D_93, D_94, D_96, D_127 : no missing values\n",
    "\n",
    "- D_42, D_49, D_66, D_73, D_76, D_87, D_88, D_106, D_108, D_110, D_111, D_132, D_134:D_138, D_142 : missing > 75% of values.\n",
    "\n",
    "- D_41, D_44:D_46, D_48, D_52, D_54:D_55, D_59, D_61, D_62, D_64, D_68:D_70, D_72, D_74, D_78:D_81, D_83, D_84, D_89, D_91, D_102:D_104, D_107, D_109, D_112:D_126, D_128:D_131, D_133, D_139:D_145: missing < 25%\n",
    "\n",
    "- D_43, D_50, D_53 D_56, D_77, D_82, D_105 : 25-75% missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq.D_63.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f930a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq.D_64.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955709d",
   "metadata": {},
   "source": [
    "**Payment Variables**\n",
    "\n",
    "- 3 total columns (P_2, P_3, P_4)\n",
    "\n",
    "- all: float\n",
    "\n",
    "- P_4 : no missing values\n",
    "\n",
    "- P_2 & P_3 : missing < 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d286d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7154901",
   "metadata": {},
   "source": [
    "**Balance Variables**\n",
    "\n",
    "- 40 variables\n",
    "\n",
    "- B_31: int (0, 1)\n",
    "\n",
    "- all others: float\n",
    "\n",
    "- B_29, B_39, and B_42 are majority null\n",
    "\n",
    "- B_17 is missing \n",
    "\n",
    "- B_1, B_4, B_5, B_7, B_9, B_10, B_11, B_12, B_14, B_18, B_21, B_23, B_24, B_28, B_31, B_32, B_36 have no missing values. \n",
    "\n",
    "- B_2, B_3, B_6, B_8, B_13, B_15, B_16, B_19, B_20, B_25, B_26, B_27, B_30, B_33, B_37, B_38, B_40, B_41 are missing < 1% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.B_31.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7db81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe08257",
   "metadata": {},
   "source": [
    "**Risk Variables**\n",
    "\n",
    "- 28 Columns\n",
    "\n",
    "- All: float\n",
    "\n",
    "- R_9, R_26: missing > 90% of values. \n",
    "\n",
    "- R_12, R_20, and R_27 are missing < 1%\n",
    "\n",
    "- R_1:R_8, R_10:R_11, R13:R19, R21:R26, R28 :  no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa46f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lists of column names by datatype for future use in analysis\n",
    "object_cols = ['D_63', 'D_64']\n",
    "int_cols = ['B_31']\n",
    "date_cols = ['S_2']\n",
    "\n",
    "# list of non_float columns in order to generate a list of all float column names (186 columns)\n",
    "non_float_cols = object_cols + int_cols + date_cols\n",
    "float_cols = [col for col in X_df.columns if col not in non_float_cols]\n",
    "len(float_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None,):\n",
    "    print(null_df.sort_values('total_nulls'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393a807",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.groupby('feature_category').percent_nulls.agg(['mean', 'median', 'max', 'min']).sort_values('mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.target.value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
